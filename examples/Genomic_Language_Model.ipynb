{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Genomic_Language_Model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOf3ToT2+g3BJNWPiLvicC3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katarinagresova/genomic_ML_playground/blob/main/Genomic_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPSBWeP--Tk4"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTtWOpv3-Vk_"
      },
      "source": [
        "!pip install biopython\n",
        "!pip install fastai --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF9MmFmBmTlG"
      },
      "source": [
        "On March 15th I was not able to run SubwordTokenizer without installing this manualy first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euXNVN_kmPFq",
        "outputId": "0eb43bea-df40-470e-cc42-d2b93a0a761c"
      },
      "source": [
        "!pip install sentencepiece!=0.1.90,!=0.1.91"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece!=0.1.90,!=0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWxnWzCzENGp",
        "outputId": "70aa1c44-7440-4582-887f-2cc716ca9896"
      },
      "source": [
        "!pip show fastai"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: fastai\n",
            "Version: 2.2.7\n",
            "Summary: fastai simplifies training fast and accurate neural nets using modern best practices\n",
            "Home-page: https://github.com/fastai/fastai/tree/master/\n",
            "Author: Jeremy Howard, Sylvain Gugger, and contributors\n",
            "Author-email: info@fast.ai\n",
            "License: Apache Software License 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: pandas, scikit-learn, torchvision, torch, packaging, spacy, pip, fastcore, requests, pyyaml, scipy, fastprogress, matplotlib, pillow\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-qGQ4knjqMF"
      },
      "source": [
        "from Bio import SeqIO\n",
        "from fastai.text.all import *"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLIdViFu-XMR"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5FxJumA9_WQ"
      },
      "source": [
        "Get data that we will use for our language model. For now, we will use human abinition cDNA, since it is small enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6uCCPq9BWuk",
        "outputId": "a1780bc4-9bf0-41f5-ed7e-9475754c28e2"
      },
      "source": [
        "!wget http://ftp.ensembl.org/pub/release-103/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.abinitio.fa.gz\n",
        "!gunzip Homo_sapiens.GRCh38.cdna.abinitio.fa.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-15 06:23:18--  http://ftp.ensembl.org/pub/release-103/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.abinitio.fa.gz\n",
            "Resolving ftp.ensembl.org (ftp.ensembl.org)... 193.62.193.139\n",
            "Connecting to ftp.ensembl.org (ftp.ensembl.org)|193.62.193.139|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20601482 (20M) [application/octet-stream]\n",
            "Saving to: ‘Homo_sapiens.GRCh38.cdna.abinitio.fa.gz’\n",
            "\n",
            "Homo_sapiens.GRCh38 100%[===================>]  19.65M   654KB/s    in 32s     \n",
            "\n",
            "2021-03-15 06:23:50 (633 KB/s) - ‘Homo_sapiens.GRCh38.cdna.abinitio.fa.gz’ saved [20601482/20601482]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwbm96ecBrma"
      },
      "source": [
        "Parse sequences from fasta file into list of sequences. Also put everything to lowercase in case there are mixed upper and lowercase. We don't want our model do learn that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJChAe1HBDNL"
      },
      "source": [
        "with open(\"Homo_sapiens.GRCh38.cdna.abinitio.fa\", \"rt\") as handle:\n",
        "  txts = L(str(record.seq).lower() for record in SeqIO.parse(handle, \"fasta\"))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-89usobMBxQd"
      },
      "source": [
        "We have 51756 sequences, together 64 739 432 characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEPu1oeaBkP4",
        "outputId": "d91de31e-16e8-4666-8e41-6fd86e2510fc"
      },
      "source": [
        "print(len(txts))\n",
        "print(len(''.join(txts)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51756\n",
            "64739432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phOo3gRDB3r0"
      },
      "source": [
        "Lets look at first sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "QaFMx0_LB6tA",
        "outputId": "397a74c9-594c-45fe-8fe9-9408981f7b8e"
      },
      "source": [
        "txts[0]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'atggaaagaggaaagaagaaaagaatttccaataagttacaacaaacttttcaccattctaaagaacccactttccttatcaaccaagctgggcttctctctagtgactcctattctagcctttccccagaaacagagagtgttaatcctggtgaaaatataaagacagacactcagaaaaagagacctgggactgtgatactatcaaaactgtcaagtagaagaattatatcggaaagccagcttagcccccctgtgatcccggcccgcaggcctggattccgggtatgctatatctgtggccgagaatttgggtcccagtcaattgccattcatgaaccccagtgcttgcagaagtggcatattgaaaacagcaagttgcccaagcatttgaggaggccagaaccctccaaaccacagtctctcagcagcagtgggtcctacagtcttcaggcaactaacgaggctgcatttcagagtgcccaggctcagctgctgccctgtgaatcctgtggccgcacattcttgccagatcatcttcttgttcatcacagaagctgcaagccaaagggtgagggtcccagagcaccacactcaaacagttctgatcatcttactggcctcaagaaagcttgtagtggaaccccagcccgaccaaggactgttatctgctacatatgtggtaaggaatttggcaccctgtcccttcctattcatgagcccaaatgcctggaaaagtggaaaatggaaaatgaccggctccctgtggagctccaccagccactcccacagaagcctcagccccttccgaatgcacagtccagccaagcgggaccaaatcaagctcagcttgtgttctgcccacattgtagccgaatctttacctcagaccgcctcctggtacaccagagaagttgtaaaactcatccttatgggccaaaatatcagaatttgaatttagggagtaaaggaggcctaaaagagtacactaattccaagcagcaaaggaacagggcagcacccagtgtaactgataaggtaattcatgccacacaagacgcattaggtgaacctggtggtgccctctgcctgtag'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXgPStbirRt"
      },
      "source": [
        "Take 1001st sequence for later testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oswI1SMrWkE4"
      },
      "source": [
        "txt = txts[1001]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71X6YELqFUwa"
      },
      "source": [
        "For even quicker work, lets use just 1000 sequences for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T25V8LrHFaj0"
      },
      "source": [
        "txts = txts[:1000]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU8szxwRDIM0"
      },
      "source": [
        "# Language model in fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sGin2w2uEIn"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "_gGspC56Rzm8",
        "outputId": "d4142ffc-5176-4790-c281-e9de1e95e89b"
      },
      "source": [
        "VOCAB_SIZE = 1000\n",
        "tokenizer = SubwordTokenizer(vocab_sz=VOCAB_SIZE)\n",
        "tokenizer.setup(txts)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sp_model': Path('tmp/spm.model')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b1Xq6imnrPJw",
        "outputId": "db7d3997-e07f-490f-dc55-a26e8bf86248"
      },
      "source": [
        "txt[:100]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'atgcatcagcagccttcggcaaaggggaaacaccgtgcagcagggctgacttggcaaagaggcacccccagaggaatgcgagggcttagagctgcaggca'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgjsbzvfamci",
        "outputId": "7c00f4df-7351-476b-d9aa-43c454fe3912"
      },
      "source": [
        "toks = first(tokenizer([txt]))\n",
        "print(coll_repr(toks, 30))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#140) ['▁atg','catca','gcag','ccttc','ggcaaa','gg','ggaaa','cacc','gtgcag','cagg','gctg','act','tggc','aaaga','ggcacc','cccag','aggaa','tgc','gaggg','cttag','agctg','caggca','atgcag','ttgct','acac','ggta','tctcag','tgt','caaga','atca'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWq48ZmFmt-H"
      },
      "source": [
        "Add Tokenizer on top of SubWordTokenizer. Not sure why this is needed, but I wasn't able to run it without this step.\n",
        "\n",
        "I set `rules=[]` so no default rules will be applied - expecialy no encoding of repeating characters.\n",
        "\n",
        "But maybe in future, some custom tokenizer with just special token for start of sequence would be nice. And for unkonown base - N."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7qAgdXXWZYc",
        "outputId": "394b5e11-e82f-439a-bc6a-118d80216620"
      },
      "source": [
        "tkn = Tokenizer(tokenizer, rules=[], sep='')\n",
        "print(coll_repr(tkn(txt), 31))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#140) ['▁atg','catca','gcag','ccttc','ggcaaa','gg','ggaaa','cacc','gtgcag','cagg','gctg','act','tggc','aaaga','ggcacc','cccag','aggaa','tgc','gaggg','cttag','agctg','caggca','atgcag','ttgct','acac','ggta','tctcag','tgt','caaga','atca','acag'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-Is2PZ9n3sW",
        "outputId": "0f2ec4c6-c840-4d15-b871-0f8bcad4b700"
      },
      "source": [
        "toks_all = txts.map(tkn)\n",
        "toks_all[0]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁atgg',\n",
              " 'aaaga',\n",
              " 'ggaa',\n",
              " 'agaaga',\n",
              " 'aaagaa',\n",
              " 'tttcca',\n",
              " 'ataa',\n",
              " 'gttac',\n",
              " 'aaca',\n",
              " 'aacttt',\n",
              " 'tcacca',\n",
              " 'ttct',\n",
              " 'aaagaa',\n",
              " 'ccca',\n",
              " 'ctttcc',\n",
              " 'ttat',\n",
              " 'caa',\n",
              " 'ccaag',\n",
              " 'ctggg',\n",
              " 'cttct',\n",
              " 'ctct',\n",
              " 'agtga',\n",
              " 'ctccta',\n",
              " 'ttcta',\n",
              " 'gcct',\n",
              " 'ttccc',\n",
              " 'cagaaa',\n",
              " 'cagag',\n",
              " 'agtg',\n",
              " 'ttaa',\n",
              " 'tcctgg',\n",
              " 'tgaaa',\n",
              " 'atat',\n",
              " 'aaaga',\n",
              " 'cagaca',\n",
              " 'ctc',\n",
              " 'agaaa',\n",
              " 'aagaga',\n",
              " 'cctggg',\n",
              " 'ac',\n",
              " 'tgtga',\n",
              " 'tactat',\n",
              " 'caaa',\n",
              " 'actg',\n",
              " 'tcaagt',\n",
              " 'agaaga',\n",
              " 'att',\n",
              " 'atat',\n",
              " 'cgg',\n",
              " 'aaagcca',\n",
              " 'gctt',\n",
              " 'agcc',\n",
              " 'cccc',\n",
              " 'tgtga',\n",
              " 'tcccg',\n",
              " 'gccc',\n",
              " 'gcagg',\n",
              " 'cctgg',\n",
              " 'att',\n",
              " 'ccggg',\n",
              " 'tatg',\n",
              " 'ctat',\n",
              " 'atct',\n",
              " 'gtggcc',\n",
              " 'gaga',\n",
              " 'attt',\n",
              " 'gggt',\n",
              " 'cccag',\n",
              " 'tcaa',\n",
              " 'ttgcc',\n",
              " 'att',\n",
              " 'catgaac',\n",
              " 'cccag',\n",
              " 'tgct',\n",
              " 'tg',\n",
              " 'cagaag',\n",
              " 'tggcat',\n",
              " 'attga',\n",
              " 'aaa',\n",
              " 'cagcaa',\n",
              " 'gttg',\n",
              " 'cccaag',\n",
              " 'cattt',\n",
              " 'gaggag',\n",
              " 'gccag',\n",
              " 'aacc',\n",
              " 'ctcc',\n",
              " 'aaac',\n",
              " 'cacag',\n",
              " 'tctct',\n",
              " 'cagcagc',\n",
              " 'agtgg',\n",
              " 'gtcc',\n",
              " 'tacag',\n",
              " 'tctt',\n",
              " 'cagg',\n",
              " 'caac',\n",
              " 'taa',\n",
              " 'cgaggc',\n",
              " 'tgc',\n",
              " 'attt',\n",
              " 'cagag',\n",
              " 'tgccca',\n",
              " 'ggct',\n",
              " 'cagctg',\n",
              " 'ctgccc',\n",
              " 'tgtga',\n",
              " 'atcct',\n",
              " 'gtggcc',\n",
              " 'gca',\n",
              " 'ca',\n",
              " 'ttctt',\n",
              " 'gccag',\n",
              " 'atca',\n",
              " 'tcttc',\n",
              " 'ttgt',\n",
              " 'tcatca',\n",
              " 'cagaag',\n",
              " 'ctgc',\n",
              " 'aagcc',\n",
              " 'aaagg',\n",
              " 'gtga',\n",
              " 'gggt',\n",
              " 'cccagag',\n",
              " 'cacc',\n",
              " 'acac',\n",
              " 'tcaaa',\n",
              " 'cagt',\n",
              " 'tctga',\n",
              " 'tcatc',\n",
              " 'ttact',\n",
              " 'ggcctc',\n",
              " 'aagaa',\n",
              " 'agct',\n",
              " 'tgta',\n",
              " 'gtgga',\n",
              " 'accc',\n",
              " 'cagccc',\n",
              " 'gac',\n",
              " 'caaggac',\n",
              " 'tgtt',\n",
              " 'atct',\n",
              " 'gctac',\n",
              " 'atat',\n",
              " 'gtggt',\n",
              " 'aagga',\n",
              " 'attt',\n",
              " 'ggcacc',\n",
              " 'ctgtcc',\n",
              " 'cttcc',\n",
              " 'tattc',\n",
              " 'atga',\n",
              " 'gccc',\n",
              " 'aaatg',\n",
              " 'cctgg',\n",
              " 'aaaagt',\n",
              " 'ggaa',\n",
              " 'aatggaa',\n",
              " 'aatga',\n",
              " 'ccgg',\n",
              " 'ctccc',\n",
              " 'tgtgga',\n",
              " 'gctcca',\n",
              " 'ccagcc',\n",
              " 'actc',\n",
              " 'ccaca',\n",
              " 'gaagc',\n",
              " 'ctcagcc',\n",
              " 'ccttc',\n",
              " 'cg',\n",
              " 'aat',\n",
              " 'gcacag',\n",
              " 'tccag',\n",
              " 'ccaag',\n",
              " 'cggg',\n",
              " 'accaa',\n",
              " 'atca',\n",
              " 'agct',\n",
              " 'cagctt',\n",
              " 'gtgt',\n",
              " 'tctgc',\n",
              " 'ccaca',\n",
              " 'ttgt',\n",
              " 'agcc',\n",
              " 'ga',\n",
              " 'atctt',\n",
              " 'tacct',\n",
              " 'caga',\n",
              " 'ccgcc',\n",
              " 'tcctgg',\n",
              " 'tac',\n",
              " 'acca',\n",
              " 'gagaag',\n",
              " 'ttgt',\n",
              " 'aaaac',\n",
              " 'tcatc',\n",
              " 'ctt',\n",
              " 'atgg',\n",
              " 'gc',\n",
              " 'caaa',\n",
              " 'atat',\n",
              " 'cagaa',\n",
              " 'tttga',\n",
              " 'attt',\n",
              " 'aggg',\n",
              " 'agtaa',\n",
              " 'aggagg',\n",
              " 'cct',\n",
              " 'aaaa',\n",
              " 'gagt',\n",
              " 'acact',\n",
              " 'aatt',\n",
              " 'ccaag',\n",
              " 'cagcaa',\n",
              " 'aggaa',\n",
              " 'caggg',\n",
              " 'cagca',\n",
              " 'cccag',\n",
              " 'tgtaa',\n",
              " 'ctga',\n",
              " 'taa',\n",
              " 'gg',\n",
              " 'taa',\n",
              " 'ttcat',\n",
              " 'gccac',\n",
              " 'acaa',\n",
              " 'gacg',\n",
              " 'catt',\n",
              " 'aggtga',\n",
              " 'acctg',\n",
              " 'gtggt',\n",
              " 'gccc',\n",
              " 'tctgc',\n",
              " 'ctg',\n",
              " 'tag']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FHmEQEtuIB9"
      },
      "source": [
        "## Numericalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYcGQdbMtcH1"
      },
      "source": [
        "Let's translate tokens into numbers.\n",
        "\n",
        "I have here representation of special 'xx' tokens. It is ok for now, but it would be nice to get rid of them in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "swmZENhkS2Ka",
        "outputId": "e33c3b17-d695-4a90-84a3-0c5cd9f17b92"
      },
      "source": [
        "num = Numericalize()\n",
        "num.setup(toks_all)\n",
        "coll_repr(num.vocab,20)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#1000) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','tga','taa','tag','gca','cacc','ga','tcctca','ctga','tcag','atga','gtcc'...]\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHVLSCaOtxLb"
      },
      "source": [
        "This is numerical representation of our testing sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu5hMzzLWKyE",
        "outputId": "0e265bae-e6e8-4896-b079-b89d5eec82d5"
      },
      "source": [
        "nums = num(toks)[:20]; nums"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorText([162, 181, 170,  38, 689, 243,  53,  13, 878,  27, 208, 160, 149,  73,\n",
              "        373,  21, 677, 147, 226, 843])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DRCY3WRt29b"
      },
      "source": [
        "Let's put it back to letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Bm7T5FH3WQ_N",
        "outputId": "d0d003e7-e86d-4dda-bbd1-ba9a7f75c4aa"
      },
      "source": [
        "''.join(num.vocab[o] for o in nums)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'▁atgcatcagcagccttcggcaaaggggaaacaccgtgcagcagggctgacttggcaaagaggcacccccagaggaatgcgagggcttag'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRQgzvQZt8Hs"
      },
      "source": [
        "And compare with original sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E5K62GXaqcjD",
        "outputId": "1d6f2572-9211-400c-f7a7-799bf773108b"
      },
      "source": [
        "txt[:100]"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'atgcatcagcagccttcggcaaaggggaaacaccgtgcagcagggctgacttggcaaagaggcacccccagaggaatgcgagggcttagagctgcaggca'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x34UFH7uynw"
      },
      "source": [
        "## DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfMAZALQxIi4"
      },
      "source": [
        "nums_all = toks_all.map(num)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsAzd4fSyPB_"
      },
      "source": [
        "Creating LMDataLoader on top of numericalized tokenize_csv\n",
        "\n",
        "This data loader should do propper splitting and padding for us. But parameters are just default for now. Investigation of more suitable parameter for genomics purposes is needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfFvTc-TwxMa"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "dl = LMDataLoader(nums_all, bs=BATCH_SIZE, seq_len=72, shuffle=False)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2lOkjhcxORm",
        "outputId": "ff43dc66-c3b7-45fa-ab14-884e1399d2b9"
      },
      "source": [
        "x,y = first(dl)\n",
        "x.shape,y.shape"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 72]), torch.Size([64, 72]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJj4-o63yeza"
      },
      "source": [
        "This is our first intependent variable - what we show to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "xhlyLIo7xRKX",
        "outputId": "94560873-8e83-4541-dba5-caaea5fc9596"
      },
      "source": [
        "' '.join(num.vocab[o] for o in x[0][:20])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'▁atgg aaaga ggaa agaaga aaagaa tttcca ataa gttac aaca aacttt tcacca ttct aaagaa ccca ctttcc ttat caa ccaag ctggg cttct'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_F_ujl-yotI"
      },
      "source": [
        "And this is out first dependent variable - what we expect out model to predict. It is the same as independent variable, just shifted by one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "wW9XHsBExU1a",
        "outputId": "55eaea0b-39e8-4f7d-df6f-66634482b39b"
      },
      "source": [
        "' '.join(num.vocab[o] for o in y[0][:20])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aaaga ggaa agaaga aaagaa tttcca ataa gttac aaca aacttt tcacca ttct aaagaa ccca ctttcc ttat caa ccaag ctggg cttct ctct'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbG4kzHt8YFQ"
      },
      "source": [
        "## Token + Numer + Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My6W2nXRERMM"
      },
      "source": [
        "I got lost and wasn't able to make LMDataLoader work for training. But I was able to put all of previous step into few lines of code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OlOav6oPCcEg",
        "outputId": "3911ed06-e964-4226-9bf7-1685fe6c5d84"
      },
      "source": [
        "splits = [list(range_of(txts[:800])), list(range(len(txts[:800]), len(txts)))]\n",
        "tfms = [Tokenizer(SubwordTokenizer(vocab_sz=VOCAB_SIZE), rules=[], sep=''), Numericalize()]\n",
        "dsets = Datasets(txts, [tfms], splits=splits, dl_type=LMDataLoader)\n",
        "\n",
        "batch_size = 64\n",
        "sequence_length = 72\n",
        "dataloaders = dsets.dataloaders(bs=batch_size, seq_len=sequence_length, is_lm=True)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDC5REW6zL9l"
      },
      "source": [
        "## Language model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdPHs20vzNeP"
      },
      "source": [
        "class LMModel_LSTM(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "        self.drop = nn.Dropout(p)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h_o.weight = self.i_h.weight\n",
        "        self.h = [torch.zeros(n_layers, BATCH_SIZE, n_hidden) for _ in range(2)]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        raw,h = self.rnn(self.i_h(x), self.h)\n",
        "        out = self.drop(raw)\n",
        "        self.h = [h_.detach() for h_ in h]\n",
        "        return self.h_o(out),raw,out\n",
        "    \n",
        "    def reset(self): \n",
        "        for h in self.h: h.zero_()"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncq6VEcPzRSn"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "pwoOZZDazSwC",
        "outputId": "220eba5e-8322-4609-9549-ca04ae3d53eb"
      },
      "source": [
        "learn = TextLearner(dataloaders, LMModel_LSTM(VOCAB_SIZE, 64, 2, 0.4),\n",
        "                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
        "learn.fit_one_cycle(15, 1e-2, wd=0.1)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>6.230824</td>\n",
              "      <td>5.303280</td>\n",
              "      <td>0.089740</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.892589</td>\n",
              "      <td>4.060067</td>\n",
              "      <td>0.106195</td>\n",
              "      <td>00:32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.237363</td>\n",
              "      <td>3.882589</td>\n",
              "      <td>0.110922</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.946543</td>\n",
              "      <td>3.789527</td>\n",
              "      <td>0.112913</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.801299</td>\n",
              "      <td>3.751593</td>\n",
              "      <td>0.112347</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.724945</td>\n",
              "      <td>3.725109</td>\n",
              "      <td>0.112734</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.670852</td>\n",
              "      <td>3.707895</td>\n",
              "      <td>0.114819</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.644575</td>\n",
              "      <td>3.700676</td>\n",
              "      <td>0.114300</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.629875</td>\n",
              "      <td>3.695015</td>\n",
              "      <td>0.114149</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.609823</td>\n",
              "      <td>3.682459</td>\n",
              "      <td>0.115895</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.601195</td>\n",
              "      <td>3.676525</td>\n",
              "      <td>0.116593</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.587020</td>\n",
              "      <td>3.671509</td>\n",
              "      <td>0.117697</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.572704</td>\n",
              "      <td>3.668855</td>\n",
              "      <td>0.117065</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>3.570549</td>\n",
              "      <td>3.667391</td>\n",
              "      <td>0.117225</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>3.550092</td>\n",
              "      <td>3.666546</td>\n",
              "      <td>0.116989</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}