{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "natural-session",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "competent-school",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fastai\n",
      "Version: 2.2.7\n",
      "Summary: fastai simplifies training fast and accurate neural nets using modern best practices\n",
      "Home-page: https://github.com/fastai/fastai/tree/master/\n",
      "Author: Jeremy Howard, Sylvain Gugger, and contributors\n",
      "Author-email: info@fast.ai\n",
      "License: Apache Software License 2.0\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: matplotlib, fastprogress, pyyaml, pip, torch, torchvision, pillow, spacy, packaging, fastcore, requests, scikit-learn, scipy, pandas\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "located-counter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: biopython\n",
      "Version: 1.78\n",
      "Summary: Freely available tools for computational molecular biology.\n",
      "Home-page: https://biopython.org/\n",
      "Author: The Biopython Contributors\n",
      "Author-email: biopython@biopython.org\n",
      "License: UNKNOWN\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: numpy\n",
      "Required-by: bio\n"
     ]
    }
   ],
   "source": [
    "!pip show biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chief-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-centre",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "offshore-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘Homo_sapiens.GRCh38.cdna.abinitio.fa.gz’ already there; not retrieving.\n",
      "\n",
      "gzip: Homo_sapiens.GRCh38.cdna.abinitio.fa already exists;\tnot overwritten\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://ftp.ensembl.org/pub/release-103/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.abinitio.fa.gz\n",
    "!yes n | gunzip Homo_sapiens.GRCh38.cdna.abinitio.fa.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-construction",
   "metadata": {},
   "source": [
    "# Token preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-prison",
   "metadata": {},
   "source": [
    "Parse sequences from fasta file into list of sequences. Also put everything to lowercase in case there are mixed upper and lowercase. We don't want our model do learn that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "superb-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Homo_sapiens.GRCh38.cdna.abinitio.fa\", \"rt\") as handle:\n",
    "  txts = L(str(record.seq).lower() for record in SeqIO.parse(handle, \"fasta\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-swaziland",
   "metadata": {},
   "source": [
    "We have 51 756 sequences, together 64 739 432 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "exclusive-substance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51756\n",
      "64739432\n"
     ]
    }
   ],
   "source": [
    "print(len(txts))\n",
    "print(len(''.join(txts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-session",
   "metadata": {},
   "source": [
    "We will take first sequence for later testing and or even quicker work, lets use just 10 000 sequences for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dependent-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txts[0]\n",
    "txts = txts[1:10001]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-chancellor",
   "metadata": {},
   "source": [
    "Now we have 10 000 sequences and 12 593 978 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tight-representation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "12593978\n"
     ]
    }
   ],
   "source": [
    "print(len(txts))\n",
    "print(len(''.join(txts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-circus",
   "metadata": {},
   "source": [
    "We will create subword tokenizer and make it create vocabulary of tokens based on our input data. Key parameter here is VOCAB_SIZE. In this example we are using VOCAB_SIZE = 70, what is 64 + 2 + 4, because we want 64 base tokens + 2 tokens respresenting special characters (unknow character and start of a sequence character) + 4 tokens for letters from alphabet. This size if 64 could in theory leads to codons, since we are working with coding DNA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rational-outdoors",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sp_model': Path('tmp/vocab64/spm.model')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECIAL_TOKENS = 2\n",
    "ALPHABET = 4\n",
    "VOCAB_SIZE = 64 + SPECIAL_TOKENS + ALPHABET\n",
    "tokenizer = SubwordTokenizer(vocab_sz=VOCAB_SIZE, special_toks=[], cache_dir='tmp/vocab64', lang='dna')\n",
    "tokenizer.setup(txts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-smooth",
   "metadata": {},
   "source": [
    "Just to verify, that we have somehow reasonable tokes, split test sequence into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acute-progress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#396) ['▁','atg','gaa','ag','agga','aa','gaa','gaa','aaga','att','tcc','aa','taa','gtt','aca','aca','aa','ctt','t','tca','cca','ttc','taa','ag','aa','ccc','act','ttc','ctt','atc'...]\n"
     ]
    }
   ],
   "source": [
    "toks = first(tokenizer([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-ireland",
   "metadata": {},
   "source": [
    "And print first 100 characters of our test sequence to compare it with tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "binary-class",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atggaaagaggaaagaagaaaagaatttccaataagttacaacaaacttttcaccattctaaagaacccactttccttatcaaccaagctgggcttctct'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-leadership",
   "metadata": {},
   "source": [
    "Add Tokenizer on top of SubWordTokenizer. Not sure why this is needed, but I wasn't able to run it without this step.\n",
    "\n",
    "I set rules=[] so no default rules will be applied - expecialy no encoding of repeating characters.\n",
    "\n",
    "But maybe in future, some custom tokenizer with just special token for start of sequence would be nice. And for unkonown base - N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "narrow-engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#396) ['▁','atg','gaa','ag','agga','aa','gaa','gaa','aaga','att','tcc','aa','taa','gtt','aca','aca','aa','ctt','t','tca','cca','ttc','taa','ag','aa','ccc','act','ttc','ctt','atc'...]\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer(tokenizer, rules=[], sep='')\n",
    "print(coll_repr(tkn(txt), 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-newark",
   "metadata": {},
   "source": [
    "Now we will tokenize all 10 000 sequences using our predefined vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "surrounded-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_all = txts.map(tkn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-figure",
   "metadata": {},
   "source": [
    "# Tokens analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-radar",
   "metadata": {},
   "source": [
    "Put tokens from all of our training sequences into one big list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fatal-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "tokens = reduce(add, toks_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-backing",
   "metadata": {},
   "source": [
    "Our sequences where splitted into 4 428 609 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "subjective-barrier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4428609"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-christmas",
   "metadata": {},
   "source": [
    "Print top 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "seventh-compatibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ag', 288473), ('aa', 200007), ('t', 195147), ('atg', 146084), ('gcc', 138483), ('gtg', 127102), ('aca', 116464), ('ccc', 114866), ('cca', 111307), ('tca', 107653)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "elements_count = collections.Counter(tokens)\n",
    "print(elements_count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-hospital",
   "metadata": {},
   "source": [
    "When we combine 3 most frequent tokens, we get 2 stop codons: TAG, TAA (DNA equivalents to RNA stop codons UAG, UAA). Fourth most frequent token is start codon ATG.\n",
    "We had only 10 000 sequences, but number of occurences of ATG is 146 084, meaning, that it occured in every sequence on average 14 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-fleece",
   "metadata": {},
   "source": [
    "What would be occurence count, if each codon would occure with the same probability?  \n",
    "We have 12 593 978 bases in 10 000 sequences, so 1 259 bases on average in each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "challenging-fantasy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1259.3978"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12593978 / 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-burst",
   "metadata": {},
   "source": [
    "That means that we have on average 419 codons per sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "southern-method",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419.6666666666667"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1259/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-possible",
   "metadata": {},
   "source": [
    "And each codon should occure on average 6.5 times in each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "patient-colonial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.546875"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "419/64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-yukon",
   "metadata": {},
   "source": [
    "So far, we were working with tokens, not k-mers or codons. Splitting to tokens don't have to follow reading frame. Let'c count real codons now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "pleasant-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmers = []\n",
    "n = 3\n",
    "for line in txts:\n",
    "    for i in range(0, len(line), n):\n",
    "        kmers.append(line[i:i+n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-premises",
   "metadata": {},
   "source": [
    "Just to verify this simple splitting algorithm, we will compare numer of kmers with length of all sequences combined divided by 3. It should be roughtly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "professional-prescription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4198166"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "starting-universal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4197992.666666667"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12593978/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-champagne",
   "metadata": {},
   "source": [
    "And now we can look at counts of k-mers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "constant-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gag', 162692), ('ctg', 152165), ('cag', 145085), ('aag', 125114), ('gaa', 124566), ('gcc', 115964), ('gtg', 109645), ('aaa', 97552), ('ggc', 96296), ('gac', 95774), ('ccc', 91170), ('atg', 90696), ('agc', 84738), ('gga', 84329), ('gat', 82691), ('gct', 82438), ('cct', 81335), ('acc', 81106), ('ctc', 80716), ('cca', 80394), ('ggg', 80040), ('atc', 79047), ('tcc', 77118), ('gca', 76364), ('ttc', 72229), ('aac', 69332), ('aca', 68714), ('cac', 68226), ('aat', 67716), ('tct', 65649), ('agg', 63645), ('tgg', 63080), ('ttt', 61324), ('aga', 60767), ('gtc', 59355), ('tca', 58930), ('att', 56859), ('act', 56609), ('ctt', 55658), ('caa', 54859), ('tgc', 53201), ('agt', 52180), ('ggt', 51674), ('ttg', 51504), ('tac', 49967), ('cat', 49780), ('cgg', 47473), ('tgt', 45702), ('gtt', 44397), ('cgc', 42905), ('tat', 40561), ('ccg', 34331), ('gcg', 34257), ('cta', 29293), ('gta', 28494), ('tta', 28493), ('ata', 28452), ('cga', 25687), ('acg', 25266), ('cgt', 20176), ('tcg', 19701), ('tga', 7572), ('taa', 3554), ('tag', 3209)]\n"
     ]
    }
   ],
   "source": [
    "kmer_count = collections.Counter(kmers)\n",
    "print(kmer_count.most_common(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-latino",
   "metadata": {},
   "source": [
    "Now, ATG has 90 696 occurences, what is still more occurences then just start codon. But ATG is also encoding for amino acid Methoynin.  \n",
    "Three least frequent codons are three stop codons: TGA, TAA and TAG. It is expected for them to occure only at the end of each sequence and not in the middle, because that would cause premature termination of traslation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-incident",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
