{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "considered-addiction",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wanted-teaching",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fastai\n",
      "Version: 2.2.7\n",
      "Summary: fastai simplifies training fast and accurate neural nets using modern best practices\n",
      "Home-page: https://github.com/fastai/fastai/tree/master/\n",
      "Author: Jeremy Howard, Sylvain Gugger, and contributors\n",
      "Author-email: info@fast.ai\n",
      "License: Apache Software License 2.0\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: requests, pillow, scikit-learn, fastprogress, fastcore, matplotlib, pandas, torchvision, spacy, torch, scipy, pyyaml, pip, packaging\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dependent-religious",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: biopython\n",
      "Version: 1.78\n",
      "Summary: Freely available tools for computational molecular biology.\n",
      "Home-page: https://biopython.org/\n",
      "Author: The Biopython Contributors\n",
      "Author-email: biopython@biopython.org\n",
      "License: UNKNOWN\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: numpy\n",
      "Required-by: bio\n"
     ]
    }
   ],
   "source": [
    "!pip show biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opened-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-yahoo",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "geographic-connecticut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘Homo_sapiens.GRCh38.cdna.abinitio.fa.gz’ already there; not retrieving.\n",
      "\n",
      "gzip: Homo_sapiens.GRCh38.cdna.abinitio.fa already exists;\tnot overwritten\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://ftp.ensembl.org/pub/release-103/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.abinitio.fa.gz\n",
    "!yes n | gunzip Homo_sapiens.GRCh38.cdna.abinitio.fa.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-reverse",
   "metadata": {},
   "source": [
    "# Token preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-lighting",
   "metadata": {},
   "source": [
    "Parse sequences from fasta file into list of sequences. Also put everything to lowercase in case there are mixed upper and lowercase. We don't want our model do learn that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aerial-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Homo_sapiens.GRCh38.cdna.abinitio.fa\", \"rt\") as handle:\n",
    "  txts = L(str(record.seq).lower() for record in SeqIO.parse(handle, \"fasta\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-claim",
   "metadata": {},
   "source": [
    "We have 51 756 sequences, together 64 739 432 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "suburban-probe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51756\n",
      "64739432\n"
     ]
    }
   ],
   "source": [
    "print(len(txts))\n",
    "print(len(''.join(txts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-bubble",
   "metadata": {},
   "source": [
    "We will take first sequence for later testing and or even quicker work, lets use just 10 000 sequences for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spanish-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txts[0]\n",
    "txts = txts[1:10001]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-agriculture",
   "metadata": {},
   "source": [
    "Now we have 10 000 sequences and 12 593 978 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "liable-carry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "12593978\n"
     ]
    }
   ],
   "source": [
    "print(len(txts))\n",
    "print(len(''.join(txts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-greensboro",
   "metadata": {},
   "source": [
    "We will create subword tokenizer and make it create vocabulary of tokens based on our input data. Key parameter here is VOCAB_SIZE. In this example we are using VOCAB_SIZE = 66, what is 64 + 2, because we want 64 base tokens + 2 tokens respresenting special characters (unknow character and start of a sequence character). This size if 64 could in theory leads to codons, since we are working with coding DNA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "british-malta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sp_model': Path('tmp/vocab64/spm.model')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECIAL_TOKENS = 2\n",
    "VOCAB_SIZE = 64 + SPECIAL_TOKENS\n",
    "tokenizer = SubwordTokenizer(vocab_sz=VOCAB_SIZE, special_toks=[], cache_dir='tmp/vocab64', lang='dna')\n",
    "tokenizer.setup(txts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-massachusetts",
   "metadata": {},
   "source": [
    "Just to verify, that we have somehow reasonable tokes, split test sequence into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "egyptian-mattress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#403) ['▁','atg','gaa','ag','agga','aaga','ag','aa','aaga','att','tcc','aa','taa','gtt','aca','aca','aa','ctt','t','tca','cca','ttc','taa','ag','aa','ccc','act','ttc','ctt','atc'...]\n"
     ]
    }
   ],
   "source": [
    "toks = first(tokenizer([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-stretch",
   "metadata": {},
   "source": [
    "And print first 100 characters of our test sequence to compare it with tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "continuous-finland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atggaaagaggaaagaagaaaagaatttccaataagttacaacaaacttttcaccattctaaagaacccactttccttatcaaccaagctgggcttctct'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-pressing",
   "metadata": {},
   "source": [
    "Add Tokenizer on top of SubWordTokenizer. Not sure why this is needed, but I wasn't able to run it without this step.\n",
    "\n",
    "I set rules=[] so no default rules will be applied - expecialy no encoding of repeating characters.\n",
    "\n",
    "But maybe in future, some custom tokenizer with just special token for start of sequence would be nice. And for unkonown base - N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "popular-temple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#403) ['▁','atg','gaa','ag','agga','aaga','ag','aa','aaga','att','tcc','aa','taa','gtt','aca','aca','aa','ctt','t','tca','cca','ttc','taa','ag','aa','ccc','act','ttc','ctt','atc'...]\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer(tokenizer, rules=[], sep='')\n",
    "print(coll_repr(tkn(txt), 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-stake",
   "metadata": {},
   "source": [
    "Now we will tokenize all 10 000 sequences using our predefined vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "promising-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_all = txts.map(tkn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-protest",
   "metadata": {},
   "source": [
    "# Tokens analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-nickel",
   "metadata": {},
   "source": [
    "Put tokens from all of our training sequences into one big list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "negative-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "tokens = reduce(add, toks_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-chamber",
   "metadata": {},
   "source": [
    "Our sequences where splitted into 1 980 816 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "clean-response",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4506720"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-differential",
   "metadata": {},
   "source": [
    "Print top 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "gothic-thursday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ag', 333706), ('t', 234379), ('aa', 224096), ('atg', 149345), ('gcc', 137982), ('gtg', 132431), ('aca', 120716), ('cca', 117287), ('ccc', 111571), ('ctg', 105940)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "elements_count = collections.Counter(tokens)\n",
    "print(elements_count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-netherlands",
   "metadata": {},
   "source": [
    "TODO: interpretacia - najcastejsie su dva stop codony a potom start codon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-backup",
   "metadata": {},
   "source": [
    "TODO: pozriet sa na vobahulary a namapovat to na kodony"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-criminal",
   "metadata": {},
   "source": [
    "TODO: dat vacsiu velkost vocabulary a pozerat ake dvojice kodonov sa objavia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-egyptian",
   "metadata": {},
   "source": [
    "TODO: pozriet sa na rozlozenie sekvencii do tokenov. predpoklad bol, ze to bude po trojiciach a ak ta, bude stvorica, tak bude nasledovana dvojicou, ale tak to zatial nevyzera. Ak mam v slovniku vsetky kodony, tak potom sa moze posunut citaci ramec a stale to mam ako tokenizovat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-phenomenon",
   "metadata": {},
   "source": [
    "TODO: dat mensiu velkost vocabulary, aby som donutila vyrvorit len najcastejsie kodony, nie vsetky. Potom znova analyzovat, ako su rozsekane sekvencie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-prague",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
