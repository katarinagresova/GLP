{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3xsfwAlupIb"
      },
      "source": [
        "# PyTorch pipeline with text augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy-CaE8OvCj7"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL3qadoFusJA",
        "outputId": "66f02286-fe84-4711-cb78-6f6a571c4ecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install -qq tensorflow_addons genomic-benchmarks\n",
        "!pip install git+https://github.com/katarinagresova/GLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mteDc_HavGv5",
        "outputId": "f967ead4-f7b2-477f-de24-27cee9987104"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/genomic_benchmarks/utils/datasets.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanEnhancersCohn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import ConstantPad1d\n",
        "from pathlib import Path\n",
        "from gpl.model.cnn import CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElSQY_9jvXDp"
      },
      "source": [
        "# Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JzZIdra3vflI"
      },
      "outputs": [],
      "source": [
        "train_dset = HumanEnhancersCohn('train', version=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2s0nzwMvow1"
      },
      "source": [
        "# Train subword tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiYz-LbIwgN3"
      },
      "source": [
        "We don't want to train new model if we already have one. This step takes about X minutes in Google Colab. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "77SFv44LvtGu"
      },
      "outputs": [],
      "source": [
        "if not Path('m.model').exists():\n",
        "    spm.SentencePieceTrainer.train(sentence_iterator=iter([x[0] for x in train_dset]), model_prefix='m', vocab_size=512,)\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file='m.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4znifptw9VK"
      },
      "source": [
        "# Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DaDISdL4NB0Y"
      },
      "outputs": [],
      "source": [
        "def check_seq_lengths(dataset, tokenizer):\n",
        "    # Compute length of the longest sequence\n",
        "    max_tok_len = max([len(tokenizer(dataset[i])) for i in range(len(dataset))])\n",
        "    print(\"max_tok_len \", max_tok_len)\n",
        "    same_length = [len(tokenizer(dataset[i])) == max_tok_len for i in range(len(dataset))]\n",
        "    if not all(same_length):\n",
        "        print(\"not all sequences are of the same length\")\n",
        "\n",
        "    return max_tok_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3cVFyB8eNC7s"
      },
      "outputs": [],
      "source": [
        "def tokenize(sp, inputs, labels, augment_factor=1):\n",
        "    \"\"\"Tokenize texts and do augmentation if augment_factor is bigger then 1\n",
        "    \n",
        "    Args:\n",
        "        tokenizer (SentencePieceProcessor): trained SentencePiece tokenizer\n",
        "        dset (List[(str, int)]): List of examples\n",
        "        augment_factor (int): \n",
        "    \n",
        "    Returns:\n",
        "        tokenized_texts (List[List[str]]): List of list of indices of tokens\n",
        "    \"\"\"\n",
        "\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(labels)):\n",
        "        \n",
        "        if augment_factor == 1:\n",
        "            ys.append(labels[i])\n",
        "            xs.append([token for token in sp.encode(inputs[i])])\n",
        "        \n",
        "        elif augment_factor > 1:\n",
        "            ys.extend([labels[i] for _ in range(augment_factor)])\n",
        "\n",
        "            x = [0 for _ in range(augment_factor)]\n",
        "            for j in range(augment_factor):\n",
        "                x[j] = [token for token in sp.encode(inputs[i], out_type=int, enable_sampling=True, alpha=0.1, nbest_size=-1)]\n",
        "            xs.extend(x)\n",
        "\n",
        "        else:\n",
        "            raise(ValueError('augment_factor have to be > 0.'))\n",
        "\n",
        "    return xs, ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UpXIxWlojRjB"
      },
      "outputs": [],
      "source": [
        "def pad(tokenized_texts, max_len):\n",
        "\n",
        "  padded_texts = tokenized_texts[:]\n",
        "  for i in range(len(padded_texts)):\n",
        "      padded_texts[i] = padded_texts[i] + [0] * (max_len - len(padded_texts[i]))\n",
        "  return padded_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xAAsXtUfMspf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "def data_loader(inputs, labels, batch_size=50):\n",
        "    \"\"\"Convert data sets to torch.Tensors and load it to DataLoader.\n",
        "    \"\"\"\n",
        "    # Convert data type to torch.Tensor\n",
        "    inputs, labels = tuple(torch.tensor(data) for data in [inputs, labels])\n",
        "\n",
        "    # Create DataLoader for data\n",
        "    tensor_data = TensorDataset(inputs, labels)\n",
        "    sampler = RandomSampler(tensor_data)\n",
        "    dataloader = DataLoader(tensor_data, sampler=sampler, batch_size=batch_size)\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lJ-AVQsqOW4h"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inputs = [x[0] for x in train_dset]\n",
        "labels = [x[1] for x in train_dset]\n",
        "\n",
        "# Train Test Split\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
        "    inputs, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-31okq30nY2r",
        "outputId": "d274bfc3-67b4-451f-ee37-f441522bd1b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16674\n",
            "16674\n",
            "4169\n",
            "4169\n"
          ]
        }
      ],
      "source": [
        "print(len(train_inputs))\n",
        "print(len(train_labels))\n",
        "print(len(val_inputs))\n",
        "print(len(val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nigQLUiWNaOF",
        "outputId": "a4c74896-6be1-4d53-f9ea-945dcc85fd0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "140\n",
            "140\n",
            "140\n"
          ]
        }
      ],
      "source": [
        "train_tokens, train_labels = tokenize(sp, train_inputs, train_labels, augment_factor=1)\n",
        "val_tokens, val_labels = tokenize(sp, val_inputs, val_labels, augment_factor=1)\n",
        "\n",
        "max_len = max([len(tokenized_text) for tokenized_text in train_tokens])\n",
        "max_len = max(max_len, max([len(tokenized_text) for tokenized_text in val_tokens]))\n",
        "train_tokens = pad(train_tokens, max_len)\n",
        "val_tokens = pad(val_tokens, max_len)\n",
        "print(len(val_tokens[0]))\n",
        "\n",
        "train_tokens_augment, train_labels_augment = tokenize(sp, train_inputs, train_labels, augment_factor=10)\n",
        "\n",
        "max_len_augment = max([len(tokenized_text) for tokenized_text in train_tokens_augment])\n",
        "max_len_augment = max(max_len_augment, max([len(tokenized_text) for tokenized_text in val_tokens]))\n",
        "train_tokens_augment = pad(train_tokens_augment, max_len_augment)\n",
        "print(len(val_tokens[0]))\n",
        "val_tokens_augment = pad(val_tokens, max_len_augment)\n",
        "print(len(val_tokens[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4poIM6Mjp2Lr",
        "outputId": "48dbbe9b-42dd-4ef3-a644-6f9fedf3c3dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16674\n",
            "16674\n",
            "166740\n",
            "166740\n",
            "4169\n",
            "4169\n"
          ]
        }
      ],
      "source": [
        "print(len(train_tokens))\n",
        "print(len(train_labels))\n",
        "print(len(train_tokens_augment))\n",
        "print(len(train_labels_augment))\n",
        "print(len(val_tokens))\n",
        "print(len(val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IUjN1A8y25a",
        "outputId": "68b332b8-9c97-4435-d818-423098cb720a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "140\n",
            "286\n",
            "286\n",
            "286\n"
          ]
        }
      ],
      "source": [
        "print(len(train_tokens[0]))\n",
        "print(len(train_tokens_augment[0]))\n",
        "print(len(val_tokens[0]))\n",
        "print(len(val_tokens_augment[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ZFPm3rQHd93p"
      },
      "outputs": [],
      "source": [
        "# Load data to PyTorch DataLoader\n",
        "train_dataloader = data_loader(train_tokens, train_labels)\n",
        "train_dataloader_augment = data_loader(train_tokens_augment, train_labels_augment)\n",
        "val_dataloader = data_loader(val_tokens, val_labels)\n",
        "val_dataloader_augment = data_loader(val_tokens_augment, val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Original model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jumvQoTlOrxP"
      },
      "outputs": [],
      "source": [
        "max_len = max([len(tokenized_text) for tokenized_text in train_tokens])\n",
        "model = CNN(\n",
        "    number_of_classes=2,\n",
        "    vocab_size=len(sp),\n",
        "    embedding_dim=100,\n",
        "    input_len=max_len\n",
        ").to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of71r-Y_O2VR",
        "outputId": "110a0b40-9f22-4a28-ccc1-e05f37fe8feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train metrics: \n",
            " Accuracy: 68.3%, Avg loss: 0.637673 \n",
            "\n",
            "Epoch 1\n",
            "Train metrics: \n",
            " Accuracy: 70.3%, Avg loss: 0.625635 \n",
            "\n",
            "Epoch 2\n",
            "Train metrics: \n",
            " Accuracy: 74.9%, Avg loss: 0.610579 \n",
            "\n",
            "Epoch 3\n",
            "Train metrics: \n",
            " Accuracy: 76.4%, Avg loss: 0.604910 \n",
            "\n",
            "Epoch 4\n",
            "Train metrics: \n",
            " Accuracy: 77.0%, Avg loss: 0.599936 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.train(train_dataloader, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL8t9IzssWsb",
        "outputId": "e5fb2644-12a2-4fac-c954-8750fcb16414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p  2076 ; tp  1006.3442997932434 ; fp  388.09259682893753\n",
            "recall  0.484751589495782 ; precision  0.721685077489677\n",
            "num_batches 84\n",
            "correct 2715\n",
            "size 4169\n",
            "Test metrics: \n",
            " Accuracy: 0.651235, F1 score: 0.579953, Avg loss: 0.656427 \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.6512353082273926, 0.5799525130525963)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.test(val_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model with augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "TsfKvTMTQPwQ"
      },
      "outputs": [],
      "source": [
        "max_len_augment = max([len(tokenized_text) for tokenized_text in train_tokens_augment])\n",
        "model_augment = CNN(\n",
        "    number_of_classes=2,\n",
        "    vocab_size=len(sp),\n",
        "    embedding_dim=100,\n",
        "    input_len=max_len_augment\n",
        ").to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbVDa3msqczv",
        "outputId": "ab916ec6-d90a-4aa2-f0e8-dad92f2c6ff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train metrics: \n",
            " Accuracy: 68.9%, Avg loss: 0.642953 \n",
            "\n",
            "Epoch 1\n",
            "Train metrics: \n",
            " Accuracy: 67.7%, Avg loss: 0.637315 \n",
            "\n",
            "Epoch 2\n",
            "Train metrics: \n",
            " Accuracy: 70.7%, Avg loss: 0.632817 \n",
            "\n",
            "Epoch 3\n",
            "Train metrics: \n",
            " Accuracy: 71.2%, Avg loss: 0.631393 \n",
            "\n",
            "Epoch 4\n",
            "Train metrics: \n",
            " Accuracy: 69.9%, Avg loss: 0.628583 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_augment.train(train_dataloader_augment, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udszkwVFxVyI",
        "outputId": "f655432e-b057-4bc9-c59a-647abe6e21c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p  2076 ; tp  1247.8806495666504 ; fp  501.35011422634125\n",
            "recall  0.6010985787893306 ; precision  0.7133882363586922\n",
            "num_batches 84\n",
            "correct 2837\n",
            "size 4169\n",
            "Test metrics: \n",
            " Accuracy: 0.680499, F1 score: 0.652447, Avg loss: 0.650656 \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.6804989206044615, 0.652447251746604)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_augment.test(val_dataloader_augment)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SentecePiece.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
