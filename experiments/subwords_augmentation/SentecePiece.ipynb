{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3xsfwAlupIb"
      },
      "source": [
        "# PyTorch pipeline with text augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy-CaE8OvCj7"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL3qadoFusJA",
        "outputId": "66f02286-fe84-4711-cb78-6f6a571c4ecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /home/katarina/git/GLP/env/lib/python3.8/site-packages (0.1.96)\n",
            "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
            "You should consider upgrading via the '/home/katarina/git/GLP/env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
            "Collecting git+https://github.com/katarinagresova/GLP\n",
            "  Cloning https://github.com/katarinagresova/GLP to /tmp/pip-req-build-f1d7jroe\n",
            "  Running command git clone --filter=blob:none -q https://github.com/katarinagresova/GLP /tmp/pip-req-build-f1d7jroe\n",
            "  Resolved https://github.com/katarinagresova/GLP to commit 5a390144f56d0db0f4187b6e6ddf7a491ec6a04b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: biopython>=1.79 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from glp==0.0.1) (1.79)\n",
            "Requirement already satisfied: genomic_benchmarks>=0.0.6 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from glp==0.0.1) (0.0.6)\n",
            "Requirement already satisfied: torch>=1.10.0 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from glp==0.0.1) (1.10.2)\n",
            "Requirement already satisfied: torchtext>=0.11.1 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from glp==0.0.1) (0.11.2)\n",
            "Requirement already satisfied: numpy in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from biopython>=1.79->glp==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.23.0 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from genomic_benchmarks>=0.0.6->glp==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from genomic_benchmarks>=0.0.6->glp==0.0.1) (1.4.0)\n",
            "Requirement already satisfied: pip>=20.0.1 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from genomic_benchmarks>=0.0.6->glp==0.0.1) (21.3.1)\n",
            "Requirement already satisfied: yarl in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from genomic_benchmarks>=0.0.6->glp==0.0.1) (1.7.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from genomic_benchmarks>=0.0.6->glp==0.0.1) (4.62.3)\n",
            "Requirement already satisfied: googledrivedownloader>=0.4 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from genomic_benchmarks>=0.0.6->glp==0.0.1) (0.4)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from genomic_benchmarks>=0.0.6->glp==0.0.1) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from torch>=1.10.0->glp==0.0.1) (4.0.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from pandas>=1.1.4->genomic_benchmarks>=0.0.6->glp==0.0.1) (2021.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from pandas>=1.1.4->genomic_benchmarks>=0.0.6->glp==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from requests>=2.23.0->genomic_benchmarks>=0.0.6->glp==0.0.1) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from requests>=2.23.0->genomic_benchmarks>=0.0.6->glp==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from requests>=2.23.0->genomic_benchmarks>=0.0.6->glp==0.0.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from requests>=2.23.0->genomic_benchmarks>=0.0.6->glp==0.0.1) (1.25.11)\n",
            "Requirement already satisfied: multidict>=4.0 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from yarl->genomic_benchmarks>=0.0.6->glp==0.0.1) (6.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/katarina/git/GLP/env/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=1.1.4->genomic_benchmarks>=0.0.6->glp==0.0.1) (1.15.0)\n",
            "Building wheels for collected packages: glp\n",
            "  Building wheel for glp (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for glp: filename=glp-0.0.1-py3-none-any.whl size=10448 sha256=4a0f18ccb3534edf199b01c9d9f7cd8304c71c80423272af84d66a4cda353e02\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ik441a9b/wheels/29/b5/06/a4f295964f0f97a8449bd111e3a5e1a8923b3b31a02661ce85\n",
            "Successfully built glp\n",
            "Installing collected packages: glp\n",
            "Successfully installed glp-0.0.1\n",
            "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
            "You should consider upgrading via the '/home/katarina/git/GLP/env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install -qq tensorflow_addons genomic-benchmarks\n",
        "!pip install git+https://github.com/katarinagresova/GLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mteDc_HavGv5",
        "outputId": "f967ead4-f7b2-477f-de24-27cee9987104"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanEnhancersCohn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import ConstantPad1d\n",
        "from pathlib import Path\n",
        "from glp.models.cnn import CNN\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElSQY_9jvXDp"
      },
      "source": [
        "# Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "JzZIdra3vflI"
      },
      "outputs": [],
      "source": [
        "train_dset = HumanEnhancersCohn('train', version=0)\n",
        "test_dset = HumanEnhancersCohn('test', version=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2s0nzwMvow1"
      },
      "source": [
        "# Train subword tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiYz-LbIwgN3"
      },
      "source": [
        "We don't want to train new model if we already have one. This step takes about X minutes in Google Colab. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "77SFv44LvtGu"
      },
      "outputs": [],
      "source": [
        "model_preffix = 'm'\n",
        "model_file = model_preffix + '.model'\n",
        "\n",
        "if not Path(model_file).exists():\n",
        "    spm.SentencePieceTrainer.train(sentence_iterator=iter([x[0] for x in train_dset]), model_prefix=model_preffix, vocab_size=512,)\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file=model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4znifptw9VK"
      },
      "source": [
        "# Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3cVFyB8eNC7s"
      },
      "outputs": [],
      "source": [
        "def tokenize(sp, inputs, labels, augment_factor=1):\n",
        "    \"\"\"Tokenize texts and do augmentation if augment_factor is bigger then 1\n",
        "    \n",
        "    Args:\n",
        "        tokenizer (SentencePieceProcessor): trained SentencePiece tokenizer\n",
        "        dset (List[(str, int)]): List of examples\n",
        "        augment_factor (int): \n",
        "    \n",
        "    Returns:\n",
        "        tokenized_texts (List[List[str]]): List of list of indices of tokens\n",
        "    \"\"\"\n",
        "\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(labels)):\n",
        "        \n",
        "        if augment_factor == 1:\n",
        "            ys.append(labels[i])\n",
        "            xs.append([token for token in sp.encode(inputs[i])])\n",
        "        \n",
        "        elif augment_factor > 1:\n",
        "            ys.extend([labels[i] for _ in range(augment_factor)])\n",
        "\n",
        "            x = [0 for _ in range(augment_factor)]\n",
        "            for j in range(augment_factor):\n",
        "                # TODO: improve augmentation by https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb\n",
        "                x[j] = [token for token in sp.encode(inputs[i], out_type=int, enable_sampling=True, alpha=0.1, nbest_size=-1)]\n",
        "            xs.extend(x)\n",
        "\n",
        "        else:\n",
        "            raise(ValueError('augment_factor have to be > 0.'))\n",
        "\n",
        "    return xs, ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "UpXIxWlojRjB"
      },
      "outputs": [],
      "source": [
        "def pad(tokenized_texts, max_len):\n",
        "\n",
        "  padded_texts = tokenized_texts[:]\n",
        "  for i in range(len(padded_texts)):\n",
        "      padded_texts[i] = padded_texts[i] + [0] * (max_len - len(padded_texts[i]))\n",
        "  return padded_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xAAsXtUfMspf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "def data_loader(inputs, labels, batch_size=50):\n",
        "    \"\"\"Convert data sets to torch.Tensors and load it to DataLoader.\n",
        "    \"\"\"\n",
        "    # Convert data type to torch.Tensor\n",
        "    inputs, labels = tuple(torch.tensor(data) for data in [inputs, labels])\n",
        "\n",
        "    # Create DataLoader for data\n",
        "    tensor_data = TensorDataset(inputs, labels)\n",
        "    sampler = RandomSampler(tensor_data)\n",
        "    dataloader = DataLoader(tensor_data, sampler=sampler, batch_size=batch_size)\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_train_dataset(dset, sp, augment_factor=1, split_factor=0.2):\n",
        "    \n",
        "    inputs = [x[0] for x in dset]\n",
        "    labels = [x[1] for x in dset]\n",
        "\n",
        "    # Train Test Split\n",
        "    train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
        "        inputs, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_tokens, train_labels = tokenize(sp, train_inputs, train_labels, augment_factor=augment_factor)\n",
        "    val_tokens, val_labels = tokenize(sp, val_inputs, val_labels, augment_factor=1)\n",
        "\n",
        "    max_len = max([len(tokenized_text) for tokenized_text in train_tokens])\n",
        "    max_len = max(max_len, max([len(tokenized_text) for tokenized_text in val_tokens]))\n",
        "    train_tokens = pad(train_tokens, max_len)\n",
        "    val_tokens = pad(val_tokens, max_len)\n",
        "\n",
        "    # Load data to PyTorch DataLoader\n",
        "    train_dataloader = data_loader(train_tokens, train_labels)  \n",
        "    val_dataloader = data_loader(val_tokens, val_labels)\n",
        "\n",
        "    return train_dataloader, val_dataloader, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_test_dataset(dset, sp, max_len):\n",
        "    inputs = [x[0] for x in dset]\n",
        "    labels = [x[1] for x in dset]\n",
        "\n",
        "    train_tokens, train_labels = tokenize(sp, inputs, labels, augment_factor=1)\n",
        "    train_tokens = pad(train_tokens, max_len)\n",
        "\n",
        "    # Load data to PyTorch DataLoader\n",
        "    return data_loader(train_tokens, train_labels) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader, val_dataloader, max_len = preprocess_train_dataset(train_dset, sp)\n",
        "train_dataloader_augment, val_dataloader_augment, max_len_augment = preprocess_train_dataset(train_dset, sp, augment_factor=2)\n",
        "test_dataloader = preprocess_test_dataset(test_dset, sp, max_len=max_len)\n",
        "test_dataloader_augment = preprocess_test_dataset(test_dset, sp, max_len=max_len_augment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import dropout, nn\n",
        "\n",
        "# A simple CNN model inspired by https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks/blob/main/src/genomic_benchmarks/models/torch.py\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, number_of_classes, vocab_size, embedding_dim, input_len, dropout):\n",
        "        super(CNN, self).__init__()\n",
        "        if number_of_classes == 2:\n",
        "            number_of_output_neurons = 1\n",
        "            loss = torch.nn.functional.binary_cross_entropy_with_logits\n",
        "            output_activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise Exception(\"Not implemented for number_of_classes!=2\")\n",
        "            # number_of_output_neurons = number_of_classes\n",
        "            # loss = torch.nn.CrossEntropyLoss()\n",
        "            # output_activation = nn.Softmax(dim=)\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=16, kernel_size=8, bias=True)\n",
        "        self.norm1 = nn.BatchNorm1d(16)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=8, kernel_size=8, bias=True)\n",
        "        self.norm2 = nn.BatchNorm1d(8)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(in_channels=8, out_channels=4, kernel_size=8, bias=True)\n",
        "        self.norm3 = nn.BatchNorm1d(4)\n",
        "        self.pool3 = nn.MaxPool1d(2)\n",
        "\n",
        "        #         compute output shape of conv layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.lin1 = nn.Linear(self.count_flatten_size(input_len), 512)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lin2 = nn.Linear(512, number_of_output_neurons)\n",
        "        self.output_activation = output_activation\n",
        "        self.loss = loss\n",
        "\n",
        "    def count_flatten_size(self, input_len):\n",
        "        zeros = torch.zeros([1, input_len], dtype=torch.long)\n",
        "        x = self.embeddings(zeros)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        return x.size()[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.lin1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output_activation(x)\n",
        "        return x\n",
        "\n",
        "    def train_loop(self, dataloader, optimizer, val_dataloader):\n",
        "        for x, y in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            pred = self(x)\n",
        "            if y.shape != pred.shape:\n",
        "                y = y.unsqueeze(1)\n",
        "                y = y.float()\n",
        "            loss = self.loss(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        train_loss, train_correct = self._eval(dataloader=dataloader)\n",
        "        if val_dataloader != None:\n",
        "            val_loss, val_correct = self._eval(dataloader=val_dataloader)\n",
        "            print(f\"Train metrics: \\n Accuracy: {(100*train_correct):>0.1f}%, Avg loss: {train_loss:>8f} Val accuracy: {(100*val_correct):>0.1f}%, Val avg loss: {val_loss:>8f} \\n\")\n",
        "        else:\n",
        "            print(f\"Train metrics: \\n Accuracy: {(100*train_correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
        "\n",
        "\n",
        "    def train(self, dataloader, epochs, val_datdaloader = None):\n",
        "        optimizer = torch.optim.Adam(self.parameters())\n",
        "        for t in range(epochs):\n",
        "            print(f\"Epoch {t}\")\n",
        "            self.train_loop(dataloader, optimizer, val_datdaloader)\n",
        "\n",
        "    def _eval(self, dataloader):\n",
        "        size = dataloader.dataset.__len__()\n",
        "        num_batches = len(dataloader)\n",
        "        loss, correct = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in dataloader:\n",
        "                pred = self(X)\n",
        "                if y.shape != pred.shape:\n",
        "                    y = y.unsqueeze(1)\n",
        "                    y = y.float()\n",
        "                loss += self.loss(pred, y).item()\n",
        "                correct += (torch.round(pred) == y).sum().item()\n",
        "\n",
        "        loss /= num_batches\n",
        "        correct /= size\n",
        "\n",
        "        return loss, correct\n",
        "\n",
        "# TODO: update for multiclass classification datasets\n",
        "    def test(self, dataloader, positive_label = 1):\n",
        "        size = dataloader.dataset.__len__()\n",
        "        num_batches = len(dataloader)\n",
        "        test_loss, correct = 0, 0\n",
        "        tp, p, fp = 0, 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in dataloader:\n",
        "                pred = self(X)\n",
        "                if y.shape != pred.shape:\n",
        "                    y = y.unsqueeze(1)\n",
        "                    y = y.float()\n",
        "                test_loss += self.loss(pred, y).item()\n",
        "                correct += (torch.round(pred) == y).sum().item()\n",
        "                p += (y == positive_label).sum().item() \n",
        "                if(positive_label == 1):\n",
        "                    tp += (y * pred).sum(dim=0).item()\n",
        "                    fp += ((1 - y) * pred).sum(dim=0).item()\n",
        "                else:\n",
        "                    tp += ((1 - y) * (1 - pred)).sum(dim=0).item()\n",
        "                    fp += (y * (1 - pred)).sum(dim=0).item()\n",
        "\n",
        "        print(\"p \", p, \"; tp \", tp, \"; fp \", fp)\n",
        "        recall = tp / p\n",
        "        precision = tp / (tp + fp)\n",
        "        print(\"recall \", recall, \"; precision \", precision)\n",
        "        f1_score = 2 * precision * recall / (precision + recall)\n",
        "        \n",
        "        print(\"num_batches\", num_batches)\n",
        "        print(\"correct\", correct)\n",
        "        print(\"size\", size)\n",
        "\n",
        "        test_loss /= num_batches\n",
        "        accuracy = correct / size\n",
        "        print(f\"Test metrics: \\n Accuracy: {accuracy:>6f}, F1 score: {f1_score:>6f}, Avg loss: {test_loss:>6f} \\n\")\n",
        "        \n",
        "        return accuracy, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Original model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "jumvQoTlOrxP"
      },
      "outputs": [],
      "source": [
        "model = CNN(\n",
        "    number_of_classes=2,\n",
        "    vocab_size=len(sp),\n",
        "    embedding_dim=100,\n",
        "    input_len=max_len,\n",
        "    dropout=0.3\n",
        ").to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of71r-Y_O2VR",
        "outputId": "110a0b40-9f22-4a28-ccc1-e05f37fe8feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train metrics: \n",
            " Accuracy: 62.2%, Avg loss: 0.664582 Val accuracy: 61.3%, Val avg loss: 0.673484 \n",
            "\n",
            "Epoch 1\n",
            "Train metrics: \n",
            " Accuracy: 63.1%, Avg loss: 0.658658 Val accuracy: 61.6%, Val avg loss: 0.669294 \n",
            "\n",
            "Epoch 2\n",
            "Train metrics: \n",
            " Accuracy: 64.6%, Avg loss: 0.654869 Val accuracy: 60.6%, Val avg loss: 0.673794 \n",
            "\n",
            "Epoch 3\n",
            "Train metrics: \n",
            " Accuracy: 65.4%, Avg loss: 0.650748 Val accuracy: 61.4%, Val avg loss: 0.668497 \n",
            "\n",
            "Epoch 4\n",
            "Train metrics: \n",
            " Accuracy: 67.4%, Avg loss: 0.645921 Val accuracy: 62.4%, Val avg loss: 0.672925 \n",
            "\n",
            "Epoch 5\n",
            "Train metrics: \n",
            " Accuracy: 67.5%, Avg loss: 0.641617 Val accuracy: 62.4%, Val avg loss: 0.668769 \n",
            "\n",
            "Epoch 6\n",
            "Train metrics: \n",
            " Accuracy: 68.5%, Avg loss: 0.641635 Val accuracy: 62.9%, Val avg loss: 0.669401 \n",
            "\n",
            "Epoch 7\n",
            "Train metrics: \n",
            " Accuracy: 68.8%, Avg loss: 0.639918 Val accuracy: 61.8%, Val avg loss: 0.673261 \n",
            "\n",
            "Epoch 8\n",
            "Train metrics: \n",
            " Accuracy: 69.3%, Avg loss: 0.636802 Val accuracy: 62.2%, Val avg loss: 0.669207 \n",
            "\n",
            "Epoch 9\n",
            "Train metrics: \n",
            " Accuracy: 69.3%, Avg loss: 0.638251 Val accuracy: 62.9%, Val avg loss: 0.671094 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.train(train_dataloader, epochs=10, val_datdaloader=val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL8t9IzssWsb",
        "outputId": "e5fb2644-12a2-4fac-c954-8750fcb16414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p  3474 ; tp  1910.278869152069 ; fp  1053.4717557430267\n",
            "recall  0.5498787763822882 ; precision  0.6445477743997725\n",
            "num_batches 139\n",
            "correct 4320\n",
            "size 6948\n",
            "Test metrics: \n",
            " Accuracy: 0.621762, F1 score: 0.593462, Avg loss: 0.672600 \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.6217616580310881, 0.5934615925521959)"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.test(test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model with augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "TsfKvTMTQPwQ"
      },
      "outputs": [],
      "source": [
        "model_augment = CNN(\n",
        "    number_of_classes=2,\n",
        "    vocab_size=len(sp),\n",
        "    embedding_dim=100,\n",
        "    input_len=max_len_augment,\n",
        "    dropout=0.3\n",
        ").to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbVDa3msqczv",
        "outputId": "ab916ec6-d90a-4aa2-f0e8-dad92f2c6ff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train metrics: \n",
            " Accuracy: 60.1%, Avg loss: 0.671901 Val accuracy: 50.2%, Val avg loss: 0.701830 \n",
            "\n",
            "Epoch 1\n",
            "Train metrics: \n",
            " Accuracy: 59.9%, Avg loss: 0.671125 Val accuracy: 50.2%, Val avg loss: 0.701667 \n",
            "\n",
            "Epoch 2\n",
            "Train metrics: \n",
            " Accuracy: 62.3%, Avg loss: 0.666124 Val accuracy: 50.3%, Val avg loss: 0.699698 \n",
            "\n",
            "Epoch 3\n",
            "Train metrics: \n",
            " Accuracy: 60.2%, Avg loss: 0.668663 Val accuracy: 50.2%, Val avg loss: 0.701759 \n",
            "\n",
            "Epoch 4\n",
            "Train metrics: \n",
            " Accuracy: 63.5%, Avg loss: 0.662358 Val accuracy: 53.9%, Val avg loss: 0.691741 \n",
            "\n",
            "Epoch 5\n",
            "Train metrics: \n",
            " Accuracy: 63.1%, Avg loss: 0.660313 Val accuracy: 53.0%, Val avg loss: 0.696190 \n",
            "\n",
            "Epoch 6\n",
            "Train metrics: \n",
            " Accuracy: 63.2%, Avg loss: 0.658675 Val accuracy: 50.3%, Val avg loss: 0.698819 \n",
            "\n",
            "Epoch 7\n",
            "Train metrics: \n",
            " Accuracy: 63.4%, Avg loss: 0.657307 Val accuracy: 50.9%, Val avg loss: 0.698131 \n",
            "\n",
            "Epoch 8\n",
            "Train metrics: \n",
            " Accuracy: 65.4%, Avg loss: 0.654311 Val accuracy: 51.3%, Val avg loss: 0.696907 \n",
            "\n",
            "Epoch 9\n",
            "Train metrics: \n",
            " Accuracy: 63.4%, Avg loss: 0.656410 Val accuracy: 50.6%, Val avg loss: 0.702763 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_augment.train(train_dataloader_augment, epochs=10, val_datdaloader=val_dataloader_augment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udszkwVFxVyI",
        "outputId": "f655432e-b057-4bc9-c59a-647abe6e21c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p  3474 ; tp  1579.5565600395203 ; fp  908.4003386497498\n",
            "recall  0.45467949339076574 ; precision  0.6348809984898363\n",
            "num_batches 139\n",
            "correct 4192\n",
            "size 6948\n",
            "Test metrics: \n",
            " Accuracy: 0.603339, F1 score: 0.529879, Avg loss: 0.677870 \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.6033390903857225, 0.5298785572860427)"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_augment.test(test_dataloader_augment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparing with character model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import get_dataset\n",
        "from glp.models import CNN\n",
        "from glp.tokenizers import get_tokenizer\n",
        "from glp.tokenizers.utils import build_vocab, coll_factory, check_config, check_seq_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET = 'human_nontata_cohn'\n",
        "VOCAB_SIZE = 0\n",
        "TOKENIZER = 'character'\n",
        "KMER = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"dataset\": DATASET,\n",
        "    \"tokenizer\": TOKENIZER,\n",
        "    \"dataset_version\": 0,\n",
        "    \"epochs\": 5,\n",
        "    \"batch_size\": 32,\n",
        "    \"use_padding\": True,\n",
        "    \"force_download\": False,\n",
        "    \"run_on_gpu\": True,\n",
        "    \"number_of_classes\": 2,\n",
        "    \"embedding_dim\": 100,\n",
        "}\n",
        "check_config(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(train_dset))\n",
        "val_size = len(train_dset) - train_size\n",
        "train_data, val_data = torch.utils.data.random_split(train_dset, [train_size, val_size], generator=torch.Generator().manual_seed(42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer(config['tokenizer'])\n",
        "tokenizer.train(train_dset=train_data, vocab_size=VOCAB_SIZE, kmer=KMER)\n",
        "vocabulary = build_vocab(train_data, tokenizer, use_padding=config[\"use_padding\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "max_tok_len  502\n"
          ]
        }
      ],
      "source": [
        "# Run on GPU or CPU\n",
        "device = 'cuda' if config[\"run_on_gpu\"] and torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))\n",
        "\n",
        "max_tok_len = check_seq_lengths(dataset=train_data, tokenizer=tokenizer)\n",
        "\n",
        "# Data Loader\n",
        "collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = max_tok_len)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_char = CNN(\n",
        "    number_of_classes=config[\"number_of_classes\"],\n",
        "    vocab_size=vocabulary.__len__(),\n",
        "    embedding_dim=config[\"embedding_dim\"],\n",
        "    input_len=max_tok_len\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train metrics: \n",
            " Accuracy: 63.7%, Avg loss: 0.652409 \n",
            "\n",
            "Epoch 1\n",
            "Train metrics: \n",
            " Accuracy: 65.2%, Avg loss: 0.648000 \n",
            "\n",
            "Epoch 2\n",
            "Train metrics: \n",
            " Accuracy: 66.6%, Avg loss: 0.644181 \n",
            "\n",
            "Epoch 3\n",
            "Train metrics: \n",
            " Accuracy: 67.7%, Avg loss: 0.638699 \n",
            "\n",
            "Epoch 4\n",
            "Train metrics: \n",
            " Accuracy: 70.2%, Avg loss: 0.642637 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_char.train(train_loader, epochs=config[\"epochs\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p  2094 ; tp  1309.8123128414154 ; fp  544.2741401195526\n",
            "recall  0.625507312722739 ; precision  0.7064461911954811\n",
            "num_batches 131\n",
            "correct 2866\n",
            "size 4169\n",
            "Test metrics: \n",
            " Accuracy: 0.687455, F1 score: 0.663518, Avg loss: 0.649707 \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.6874550251858959, 0.6635175437250562)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_loader = DataLoader(val_data, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
        "\n",
        "acc, f1 = model_char.test(test_loader)\n",
        "acc, f1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SentecePiece.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
