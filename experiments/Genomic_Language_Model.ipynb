{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Genomic_Language_Model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN3/myPWGic+BgkwVNh+m9s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katarinagresova/GLP/blob/main/examples/Genomic_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPSBWeP--Tk4"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-PP0_bNGVyi"
      },
      "source": [
        "## Needed for Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTtWOpv3-Vk_"
      },
      "source": [
        "!pip install biopython\n",
        "!pip install fastai --upgrade\n",
        "!pip install sentencepiece!=0.1.90,!=0.1.91"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFE40izSGbXL"
      },
      "source": [
        "## And this everywhere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0mjJBBoGs_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c787bc0e-90af-49f5-d004-ae313a3f64ff"
      },
      "source": [
        "!pip show fastai"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: fastai\n",
            "Version: 2.2.7\n",
            "Summary: fastai simplifies training fast and accurate neural nets using modern best practices\n",
            "Home-page: https://github.com/fastai/fastai/tree/master/\n",
            "Author: Jeremy Howard, Sylvain Gugger, and contributors\n",
            "Author-email: info@fast.ai\n",
            "License: Apache Software License 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: torch, fastprogress, packaging, scipy, fastcore, pyyaml, requests, torchvision, matplotlib, pandas, scikit-learn, spacy, pip, pillow\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0j5A84SGwGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e866d998-bc7b-4956-d378-58279ef04d59"
      },
      "source": [
        "!pip show biopython"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: biopython\n",
            "Version: 1.78\n",
            "Summary: Freely available tools for computational molecular biology.\n",
            "Home-page: https://biopython.org/\n",
            "Author: The Biopython Contributors\n",
            "Author-email: biopython@biopython.org\n",
            "License: UNKNOWN\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: numpy\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-qGQ4knjqMF"
      },
      "source": [
        "from Bio import SeqIO\n",
        "from fastai.text.all import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qxYdzW3EW7t"
      },
      "source": [
        "Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUoR1XF4EWYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c1994e4-1908-47f0-888c-faa71b5a3ea4"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 1, 'Tesla T4')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjDDQpmh-i31"
      },
      "source": [
        "Download file with utils for data preparation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_tWfobc-pBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079c5631-6e2c-42fc-bc4d-a4ca32b92e23"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/katarinagresova/GLP/main/src/glp/utils.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-18 06:19:43--  https://raw.githubusercontent.com/katarinagresova/GLP/main/src/glp/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1440 (1.4K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]   1.41K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-18 06:19:43 (38.2 MB/s) - ‘utils.py’ saved [1440/1440]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLIdViFu-XMR"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5FxJumA9_WQ"
      },
      "source": [
        "Get data that we will use for our language model. For now, we will use human abinition cDNA, since it is small enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6uCCPq9BWuk",
        "outputId": "1e874fc8-9192-4961-a70a-f20eac61fa98"
      },
      "source": [
        "!wget http://ftp.ensembl.org/pub/release-103/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.abinitio.fa.gz\n",
        "!gunzip Homo_sapiens.GRCh38.cdna.abinitio.fa.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-18 06:20:06--  http://ftp.ensembl.org/pub/release-103/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.abinitio.fa.gz\n",
            "Resolving ftp.ensembl.org (ftp.ensembl.org)... 193.62.193.139\n",
            "Connecting to ftp.ensembl.org (ftp.ensembl.org)|193.62.193.139|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20601482 (20M) [application/octet-stream]\n",
            "Saving to: ‘Homo_sapiens.GRCh38.cdna.abinitio.fa.gz’\n",
            "\n",
            "Homo_sapiens.GRCh38 100%[===================>]  19.65M   477KB/s    in 43s     \n",
            "\n",
            "2021-03-18 06:20:49 (473 KB/s) - ‘Homo_sapiens.GRCh38.cdna.abinitio.fa.gz’ saved [20601482/20601482]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnTiOOxU__l6"
      },
      "source": [
        "Lets create expected folder structure for binary classification:\n",
        " - root/train/0\n",
        " - root/train/1\n",
        " - root/valid/0\n",
        " - root/valid/1\n",
        "\n",
        "Then parse our fasta file so each sequence is one txt file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RqZkVlm_l9o"
      },
      "source": [
        "import utils\n",
        "\n",
        "ROOT_DIR = 'data/cdna/'\n",
        "utils.prepare_folder_structure(ROOT_DIR)\n",
        "utils.split_fasta_to_txts('Homo_sapiens.GRCh38.cdna.abinitio.fa', ROOT_DIR, '1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v3n-JzVDJ5x"
      },
      "source": [
        "# Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6JQlZ0rCkdY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "4575d422-22eb-43f7-bbc3-d4fb3af9a550"
      },
      "source": [
        "BATCH_SIZE = 2048\n",
        "SEQ_LEN = 50\n",
        "VOCAB_SIZE = 10000\n",
        "dls_lm = TextDataLoaders.from_folder(\n",
        "    Path(ROOT_DIR), \n",
        "    bs=BATCH_SIZE, \n",
        "    seed=42, \n",
        "    is_lm=True, \n",
        "    tok_tfm=Tokenizer(SubwordTokenizer(vocab_sz=VOCAB_SIZE), rules=[], sep=''), \n",
        "    seq_len=SEQ_LEN\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMgjDeTeHQ5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "b3a29037-20f6-4498-b526-06b67564f5b8"
      },
      "source": [
        "dls_lm.show_batch()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>▁ATGTCCAACAACATGGCCAAGATTGCCGAGGCCCGCAAGACGGTGGAACAGCTGAAGCTGGAGGTGAACATCGACCGCATGAAGGTGTCGCAGGCAGCAGCGGAACTCCTGGCTTTCT</td>\n",
              "      <td>ATGTCCAACAACATGGCCAAGATTGCCGAGGCCCGCAAGACGGTGGAACAGCTGAAGCTGGAGGTGAACATCGACCGCATGAAGGTGTCGCAGGCAGCAGCGGAACTCCTGGCTTTCTGC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GCAAGTGGCGCGGCAGGCCAAGGCCTTCCTGTCGCTGGGGAAGATGGCCGAGGTGCAGGTGAGCCGGCGCCGGGCCGGCGGCGCGCAGTCCTGGCTGTGG</td>\n",
              "      <td>GTGGCGCGGCAGGCCAAGGCCTTCCTGTCGCTGGGGAAGATGGCCGAGGTGCAGGTGAGCCGGCGCCGGGCCGGCGGCGCGCAGTCCTGGCTGTGGTTC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CCTGTCATTCTTTTCCAAAAAATGGGAGTAGGTAAACTTGAGATGTATGTGCTTAATCCAGTCAAGAGCAGCAAGGAAATGCAGTATTTTATGCAGCAGTGGACTGGTACCAACA</td>\n",
              "      <td>TGTCATTCTTTTCCAAAAAATGGGAGTAGGTAAACTTGAGATGTATGTGCTTAATCCAGTCAAGAGCAGCAAGGAAATGCAGTATTTTATGCAGCAGTGGACTGGTACCAACAA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AGGGGCTGCTGCTGCTGCTGGGAATCTTCCTTGCTTATGAGACCAAGAGTGTGTCCACTGAGAAGATCAATGATCACCGGGCTGTGGGCATGGCTATCTACAATGTGGCAGTCCTGTGCCTC</td>\n",
              "      <td>GGGCTGCTGCTGCTGCTGGGAATCTTCCTTGCTTATGAGACCAAGAGTGTGTCCACTGAGAAGATCAATGATCACCGGGCTGTGGGCATGGCTATCTACAATGTGGCAGTCCTGTGCCTCATCAC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TTCACAGTCATCACGAACATCATCACCGCCACCTTAACCATCATTGCCAACATCACTACCATCACTACCACCACCACTGTTACTACTATCTGA▁ATGGTTCATGATGCTGTA</td>\n",
              "      <td>ACAGTCATCACGAACATCATCACCGCCACCTTAACCATCATTGCCAACATCACTACCATCACTACCACCACCACTGTTACTACTATCTGA▁ATGGTTCATGATGCTGTACCA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>TGAAGTGGTCCTCAGATTTCAGACGGTTCAGGTTCCTGGTGGAACCGAAGACAGCAAAGATAAGGTGCTGGTGATCAGCCTCTACTTCCTCAGGTATATCCAG</td>\n",
              "      <td>GAAGTGGTCCTCAGATTTCAGACGGTTCAGGTTCCTGGTGGAACCGAAGACAGCAAAGATAAGGTGCTGGTGATCAGCCTCTACTTCCTCAGGTATATCCAGGAAA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GATTGATTGCCTGCTTGCCCAAAAGGTTCGCCCCAGGAGGTGGAAACTTCAAGTGCTGGAAATGCGGGATGTTGATGAGAATTTTTGGACCATATGGTCTGGAGCCAGGCTCCTGTCCTGC</td>\n",
              "      <td>TTGATTGCCTGCTTGCCCAAAAGGTTCGCCCCAGGAGGTGGAAACTTCAAGTGCTGGAAATGCGGGATGTTGATGAGAATTTTTGGACCATATGGTCTGGAGCCAGGCTCCTGTCCTGCTCCC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>GAGAGCAGCTGGATATCCTGAGTGTTGGAATCCTAGTGAAAGAAAGATGGAAAGTGTTGAGAAAGATTGGGGGTGGGGGCTTTGGAGAAATTTACGATGCCTTGGACATGCTCACCAGGGAAAATGTT</td>\n",
              "      <td>AGAGCAGCTGGATATCCTGAGTGTTGGAATCCTAGTGAAAGAAAGATGGAAAGTGTTGAGAAAGATTGGGGGTGGGGGCTTTGGAGAAATTTACGATGCCTTGGACATGCTCACCAGGGAAAATGTTGC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>GTGGCAAAGGCCAAAGGCCCCAAGCTGTTGGCACCGGAAACGTCGAGGTGGAGGACGCCATGCTGGACACCTACGACCTGGTATATGAGCAGGCGATGAAAGGT</td>\n",
              "      <td>TGGCAAAGGCCAAAGGCCCCAAGCTGTTGGCACCGGAAACGTCGAGGTGGAGGACGCCATGCTGGACACCTACGACCTGGTATATGAGCAGGCGATGAAAGGTAC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGhRvukyMZuc"
      },
      "source": [
        "TODO: there are some spaces in sequences, do something about it? Or better, why is space as beggining of sequence?\n",
        "TODO: why are tokens not separated?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HmC-7D6DRN0"
      },
      "source": [
        "# Language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unLVlZKrFAAa"
      },
      "source": [
        "I am using this existing model for now, because when I tried to create my own model and run it in Colab with GPU, I got cuda/cpu mismatch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhjMe2A8DX_m"
      },
      "source": [
        "learn = language_model_learner(\n",
        "    dls_lm, AWD_LSTM, drop_mult=0.3, pretrained=False, \n",
        "    metrics=[accuracy, Perplexity()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wZsSjXCDaT6"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT3pVp3GErBd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "bcc681f9-07d8-4f40-dfb8-1b90d26f892c"
      },
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.514877</td>\n",
              "      <td>3.496706</td>\n",
              "      <td>0.076161</td>\n",
              "      <td>33.006550</td>\n",
              "      <td>30:55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCp9C0KrNSUV"
      },
      "source": [
        "We have accuracy 7.6% when predicting one of 10000 tokens as the next token in sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v84_LCMkNfS_"
      },
      "source": [
        "TODO: compare with random selection or selection based on most frequent token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE5ppoapFj2d"
      },
      "source": [
        "# Exploring tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwbm96ecBrma"
      },
      "source": [
        "Parse sequences from fasta file into list of sequences. Also put everything to lowercase in case there are mixed upper and lowercase. We don't want our model do learn that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJChAe1HBDNL"
      },
      "source": [
        "with open(\"Homo_sapiens.GRCh38.cdna.abinitio.fa\", \"rt\") as handle:\n",
        "  txts = L(str(record.seq).lower() for record in SeqIO.parse(handle, \"fasta\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-89usobMBxQd"
      },
      "source": [
        "We have 51756 sequences, together 64 739 432 characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEPu1oeaBkP4",
        "outputId": "f5eb3a48-cb30-43ec-b040-085e01827f8b"
      },
      "source": [
        "print(len(txts))\n",
        "print(len(''.join(txts)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51756\n",
            "64739432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phOo3gRDB3r0"
      },
      "source": [
        "Lets look at first sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "QaFMx0_LB6tA",
        "outputId": "75088c02-195a-42d2-85ca-ac084e7ad875"
      },
      "source": [
        "txts[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'atggaaagaggaaagaagaaaagaatttccaataagttacaacaaacttttcaccattctaaagaacccactttccttatcaaccaagctgggcttctctctagtgactcctattctagcctttccccagaaacagagagtgttaatcctggtgaaaatataaagacagacactcagaaaaagagacctgggactgtgatactatcaaaactgtcaagtagaagaattatatcggaaagccagcttagcccccctgtgatcccggcccgcaggcctggattccgggtatgctatatctgtggccgagaatttgggtcccagtcaattgccattcatgaaccccagtgcttgcagaagtggcatattgaaaacagcaagttgcccaagcatttgaggaggccagaaccctccaaaccacagtctctcagcagcagtgggtcctacagtcttcaggcaactaacgaggctgcatttcagagtgcccaggctcagctgctgccctgtgaatcctgtggccgcacattcttgccagatcatcttcttgttcatcacagaagctgcaagccaaagggtgagggtcccagagcaccacactcaaacagttctgatcatcttactggcctcaagaaagcttgtagtggaaccccagcccgaccaaggactgttatctgctacatatgtggtaaggaatttggcaccctgtcccttcctattcatgagcccaaatgcctggaaaagtggaaaatggaaaatgaccggctccctgtggagctccaccagccactcccacagaagcctcagccccttccgaatgcacagtccagccaagcgggaccaaatcaagctcagcttgtgttctgcccacattgtagccgaatctttacctcagaccgcctcctggtacaccagagaagttgtaaaactcatccttatgggccaaaatatcagaatttgaatttagggagtaaaggaggcctaaaagagtacactaattccaagcagcaaaggaacagggcagcacccagtgtaactgataaggtaattcatgccacacaagacgcattaggtgaacctggtggtgccctctgcctgtag'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXgPStbirRt"
      },
      "source": [
        "Take first sequence for later testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oswI1SMrWkE4"
      },
      "source": [
        "txt = txts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71X6YELqFUwa"
      },
      "source": [
        "For even quicker work, lets use just 10000 sequences for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T25V8LrHFaj0"
      },
      "source": [
        "txts = txts[1:10001]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sGin2w2uEIn"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miHCIgTTGjXt"
      },
      "source": [
        "Create sub-word tokenizer and make it create vocabulary of tokens based on our input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "_gGspC56Rzm8",
        "outputId": "ea7e7430-26cd-4735-9874-71faf10472d8"
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "tokenizer = SubwordTokenizer(vocab_sz=VOCAB_SIZE)\n",
        "tokenizer.setup(txts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sp_model': Path('tmp/spm.model')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiFTA_4ZJzxM"
      },
      "source": [
        "Just to verify, that we have somehow reasonable tokes, split test sequence into tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgjsbzvfamci",
        "outputId": "db49b566-a2a3-421b-87b8-234ed0e5a4db"
      },
      "source": [
        "toks = first(tokenizer([txt]))\n",
        "print(coll_repr(toks, 30))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#176) ['▁atgg','aaagagga','aagaagaaa','agaattt','ccaat','aagtt','acaacaaa','cttttc','acca','ttctaaa','gaacccac','tttcctt','atcaac','caagctg','ggcttc','tctct','agtga','ctccta','ttctag','cctttccc','cagaaa','cagagag','tgttaa','tcctgg','tgaaaat','ataaaga','cagaca','ctc','agaaaaaga','gacctggg'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9e8nSD2KZHa"
      },
      "source": [
        "And print first 100 characters of our test sequence to compare it with tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b1Xq6imnrPJw",
        "outputId": "71f56e08-73a0-4616-8c0e-94539b922394"
      },
      "source": [
        "txt[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'atggaaagaggaaagaagaaaagaatttccaataagttacaacaaacttttcaccattctaaagaacccactttccttatcaaccaagctgggcttctct'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWq48ZmFmt-H"
      },
      "source": [
        "Add Tokenizer on top of SubWordTokenizer. Not sure why this is needed, but I wasn't able to run it without this step.\n",
        "\n",
        "I set `rules=[]` so no default rules will be applied - expecialy no encoding of repeating characters.\n",
        "\n",
        "But maybe in future, some custom tokenizer with just special token for start of sequence would be nice. And for unkonown base - N."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7qAgdXXWZYc",
        "outputId": "97792add-c757-4066-c00b-2249d98d9a8e"
      },
      "source": [
        "tkn = Tokenizer(tokenizer, rules=[], sep='')\n",
        "print(coll_repr(tkn(txt), 31))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#176) ['▁atgg','aaagagga','aagaagaaa','agaattt','ccaat','aagtt','acaacaaa','cttttc','acca','ttctaaa','gaacccac','tttcctt','atcaac','caagctg','ggcttc','tctct','agtga','ctccta','ttctag','cctttccc','cagaaa','cagagag','tgttaa','tcctgg','tgaaaat','ataaaga','cagaca','ctc','agaaaaaga','gacctggg','actgtga'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Is2PZ9n3sW"
      },
      "source": [
        "toks_all = txts.map(tkn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0EVkh-1OKAB"
      },
      "source": [
        "## Frequency analysis of tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RhxD_TjOyIB"
      },
      "source": [
        "Put tokens from all of our training sequences into one big list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOiB-WzDOOw2"
      },
      "source": [
        "from operator import add\n",
        "\n",
        "tokens = reduce(add, toks_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajKbV4jUO4Nv"
      },
      "source": [
        "Our sequences where splitted into 1 980 816 tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJmvEzUVO5Gn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e81aaac-2046-429c-e781-a5c883fcf08e"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1980816"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgm6TGLpSoR3"
      },
      "source": [
        "Print top 10 most common tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsU60W5nO_0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f9ecec-9442-43ab-e624-dd71875fe7f9"
      },
      "source": [
        "import collections\n",
        "\n",
        "elements_count = collections.Counter(tokens)\n",
        "print(elements_count.most_common(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('tga', 7306), ('ctga', 3823), ('tag', 3709), ('ag', 3678), ('taa', 3328), ('ttga', 2974), ('atga', 2939), ('agtga', 2924), ('▁atg', 2798), ('tgtga', 2792)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb37aHxzSsOw"
      },
      "source": [
        "The most common token is 'tga' which is stop codon. Start codon (atg) is also in top 10, but 2x times:\n",
        " - 'atga'\n",
        " - '_atg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc4kr70aTLRi"
      },
      "source": [
        "TODO: remove spaces and try again"
      ]
    }
  ]
}